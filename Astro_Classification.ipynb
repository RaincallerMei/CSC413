{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNRue8FimE4yVjcUOXXTd+0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaincallerMei/CSC413/blob/main/Astro_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "hBEO5K031Fxw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrQ5Gr_Y3rMh",
        "outputId": "430bc6f6-8cca-43ff-d90d-a0230a9fe155"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/AstroCNN/galaxy-zoo-the-galaxy-challenge.zip -d /content/dataset\n"
      ],
      "metadata": {
        "id": "Ml97jfMw4gGc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60e1464-1ef7-4bd2-812f-b13addd91714"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/AstroCNN/galaxy-zoo-the-galaxy-challenge.zip\n",
            "  inflating: /content/dataset/all_ones_benchmark.zip  \n",
            "  inflating: /content/dataset/all_zeros_benchmark.zip  \n",
            "  inflating: /content/dataset/central_pixel_benchmark.zip  \n",
            "  inflating: /content/dataset/images_test_rev1.zip  \n",
            "  inflating: /content/dataset/images_training_rev1.zip  \n",
            "  inflating: /content/dataset/training_solutions_rev1.zip  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Go to the directory where the 6 zips live\n",
        "os.chdir(\"/content/dataset\")\n",
        "\n",
        "# Find all the .zip files\n",
        "zip_files = glob.glob(\"*.zip\")\n",
        "\n",
        "# Unzip each\n",
        "for zipf in zip_files:\n",
        "    folder_name = os.path.splitext(zipf)[0]  # e.g., \"folder1\" from \"folder1.zip\"\n",
        "    !unzip -q {zipf} -d {folder_name}"
      ],
      "metadata": {
        "id": "1hERWJ8U6oG5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.zip"
      ],
      "metadata": {
        "id": "T8LNZibG7VH8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Match Images to CSV Rows"
      ],
      "metadata": {
        "id": "GR3XkskPP74M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and Index the CSV"
      ],
      "metadata": {
        "id": "SJoOBeOoP-Ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "solutions_df = pd.read_csv(\"/content/dataset/training_solutions_rev1/training_solutions_rev1.csv\")\n",
        "\n",
        "# Suppose the columns are like:\n",
        "# GalaxyID, p1, p2, ..., p37\n",
        "# We'll create a dict { galaxy_id: [p1, p2, ..., p37], ... }\n",
        "\n",
        "id_to_probs = {}\n",
        "for row in solutions_df.itertuples(index=False):\n",
        "    galaxy_id = str(row.GalaxyID)\n",
        "    # row has p1, p2, ..., p37 as subsequent columns\n",
        "    # Convert them to a list or tensor\n",
        "    probabilities = list(row[1:])  # row[0] is GalaxyID, row[1:] are the 37 prob columns\n",
        "    id_to_probs[galaxy_id] = probabilities\n",
        "\n",
        "#Now id_to_probs[\"123456\"] might be [0.123, 0.456, ..., 0.999], etc."
      ],
      "metadata": {
        "id": "H55nfoC67hBq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Dataset Class:\n",
        "\n",
        "\n",
        "1.   Lists all image files in training/.\n",
        "2. For each file, extracts GalaxyID from the filename (e.g., 123456 from 123456.jpg).\n",
        "3. Finds the matching probabilities from id_to_probs.\n",
        "4. Loads the image, applies transformations, and returns (image_tensor, label_tensor).\n",
        "\n"
      ],
      "metadata": {
        "id": "-HNobSe0QBtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class GalaxyDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 images_dir,\n",
        "                 id_to_probs,\n",
        "                 transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.id_to_probs = id_to_probs\n",
        "        self.transform = transform\n",
        "\n",
        "        # List all JPG files in images_dir\n",
        "        self.image_paths = [\n",
        "            os.path.join(images_dir, f)\n",
        "            for f in os.listdir(images_dir)\n",
        "            if f.lower().endswith('.jpg')\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        # Filename is something like \"123456.jpg\"\n",
        "        filename = os.path.basename(img_path)\n",
        "        galaxy_id = os.path.splitext(filename)[0]  # \"123456\"\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Convert probabilities to a tensor\n",
        "        # If galaxy_id doesn't exist in id_to_probs,\n",
        "        # you might want to handle KeyError or skip it\n",
        "        probabilities = self.id_to_probs.get(galaxy_id, None)\n",
        "        if probabilities is None:\n",
        "            # If no label is found, you could handle it\n",
        "            # (e.g., raise an Exception, or return dummy data)\n",
        "            raise ValueError(f\"GalaxyID {galaxy_id} not found in id_to_probs\")\n",
        "\n",
        "        # Transform image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert list of probabilities to a FloatTensor\n",
        "        label = torch.tensor(probabilities, dtype=torch.float32)\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "2PxdCBy-7qHh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Creating a DataLoader\n"
      ],
      "metadata": {
        "id": "gJ7NNUQZQKxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Dataset and a DataLoader"
      ],
      "metadata": {
        "id": "dF0cUXLmQNDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"/content/dataset/images_training_rev1/images_training_rev1\"\n",
        "train_dataset = GalaxyDataset(\n",
        "    images_dir=train_dir,\n",
        "    id_to_probs=id_to_probs\n",
        ")\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2  # or more, depending on environment\n",
        ")\n"
      ],
      "metadata": {
        "id": "xck_nYiVQNO4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Defining the CNN Model\n"
      ],
      "metadata": {
        "id": "6cTCP6YoQphF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GOATED CNN MODEL IMPLEMENTATION"
      ],
      "metadata": {
        "id": "fditA0GPQrvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GalaxyCNN(nn.Module):\n",
        "    def __init__(self, num_outputs=37):\n",
        "        super(GalaxyCNN, self).__init__()\n",
        "        # Example: small CNN\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        # After a couple of pools, the feature map shrinks.\n",
        "        # Let's do a rough down-sampling approach:\n",
        "\n",
        "        self.fc1 = nn.Linear(32 * 56 * 56, 128)\n",
        "        self.fc2 = nn.Linear(128, num_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # shape: [batch,16,112,112]\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # shape: [batch,32,56,56]\n",
        "        x = x.view(x.size(0), -1)            # flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # shape: [batch, 37]\n",
        "        # We'll apply a sigmoid later, or use BCEWithLogitsLoss\n",
        "        return x\n",
        "\n",
        "model = GalaxyCNN(num_outputs=37)\n"
      ],
      "metadata": {
        "id": "sKcb57x-QxWX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Training: Loss Function and Optimizer\n"
      ],
      "metadata": {
        "id": "d0-W0jkEQyuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BCEWithLogitsLoss is typically used for multi-label classification of the 37 features that can be present (probability close to 1) or absent (close to 0)"
      ],
      "metadata": {
        "id": "I4_sSoCSQ1gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "#A straightforward training loop in PyTorch:\n",
        "\n",
        "num_epochs = 5\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        # images shape: [batch, 3, 224, 224]\n",
        "        # labels shape: [batch, 37]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e-sf0DaFRGIA",
        "outputId": "b95230b1-e57c-47b2-be62-0d7a3732e072"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n           ^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 240, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2bb5a842de3f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# images shape: [batch, 3, 224, 224]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# labels shape: [batch, 37]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n           ^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 240, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n"
          ]
        }
      ]
    }
  ]
}